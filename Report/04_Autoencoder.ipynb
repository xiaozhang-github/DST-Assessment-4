{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04_Autoencoder followed by FFNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this section we will apply semi-supervised learning mechanism to the network intrusion detection task, try to find best hyper-parameters and get a better performance. First we use autoencoder to do unsupervised learning with unlabeled data, and then use FFNN to do classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We first start by importing the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\py3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\anaconda\\envs\\py3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\anaconda\\envs\\py3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\anaconda\\envs\\py3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\anaconda\\envs\\py3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\anaconda\\envs\\py3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "d:\\anaconda\\envs\\py3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\anaconda\\envs\\py3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\anaconda\\envs\\py3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\anaconda\\envs\\py3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\anaconda\\envs\\py3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\anaconda\\envs\\py3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.models import load_model\n",
    "import csv\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from matplotlib import pyplot\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fix random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"CICIDS2017_Wed_train.zip\",header=None)\n",
    "test=pd.read_csv(\"CICIDS2017_Wed_test.zip\",header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split training dataset into an unlabeled one and a labeled one to do autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train75,train25=train_test_split(train,test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split into input and output variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train75x=train75.drop([78],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train25x=train25.drop([78],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train25y=train25[78]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "testx=test.drop([78],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "testy=test[78]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs=train75x.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ini=Input(shape=(n_inputs,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoder level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\anaconda\\envs\\py3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "e=Dense(n_inputs*2)(ini)\n",
    "e=BatchNormalization()(e)\n",
    "e=LeakyReLU()(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoder level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "e=Dense(n_inputs)(e)\n",
    "e=BatchNormalization()(e)\n",
    "e=LeakyReLU()(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bottleneck=n_inputs\n",
    "bottleneck=Dense(n_bottleneck)(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define decoder, level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=Dense(n_inputs)(bottleneck)\n",
    "d=BatchNormalization()(d)\n",
    "d=LeakyReLU()(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decoder level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=Dense(n_inputs*2)(d)\n",
    "d=BatchNormalization()(d)\n",
    "d=LeakyReLU()(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=Dense(n_inputs,activation='linear')(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "automodel=Model(inputs=ini,outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compile autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "automodel.compile(optimizer='adam',loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit the autoencoder model to reconstruct input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 362988 samples, validate on 207422 samples\n",
      "362988/362988 - 20s - loss: 0.1360 - val_loss: 0.2517\n"
     ]
    }
   ],
   "source": [
    "history=automodel.fit(train75x,train75x,epochs=1,batch_size=64,verbose=2,validation_data=(testx,testx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define an encoder model (without the decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=Model(inputs=ini,outputs=bottleneck)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the encoder to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.save('encoder.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encode the 25% of the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train25xen=encoder.predict(train25x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encode the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "testxen=encoder.predict(testx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyper-parameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\py3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80664/80664 [==============================] - 1s 12us/sample - loss: 0.2278 - acc: 0.9234\n",
      "40332/40332 [==============================] - 0s 6us/sample - loss: 0.1179 - acc: 0.9675\n",
      "80664/80664 [==============================] - 1s 12us/sample - loss: 0.1990 - acc: 0.9422\n",
      "40332/40332 [==============================] - 0s 6us/sample - loss: 0.0945 - acc: 0.9715\n",
      "80664/80664 [==============================] - 1s 12us/sample - loss: 0.3097 - acc: 0.9037\n",
      "40332/40332 [==============================] - 0s 6us/sample - loss: 0.0903 - acc: 0.9670\n",
      "80664/80664 [==============================] - 1s 13us/sample - loss: 0.2196 - acc: 0.9265\n",
      "40332/40332 [==============================] - 0s 7us/sample - loss: 0.1066 - acc: 0.9715\n",
      "80664/80664 [==============================] - 1s 13us/sample - loss: 0.2231 - acc: 0.9205\n",
      "40332/40332 [==============================] - 0s 7us/sample - loss: 0.1001 - acc: 0.9669\n",
      "80664/80664 [==============================] - 1s 14us/sample - loss: 0.2299 - acc: 0.9252\n",
      "40332/40332 [==============================] - 0s 7us/sample - loss: 0.0970 - acc: 0.9672\n",
      "Epoch 1/2\n",
      "80664/80664 [==============================] - 1s 13us/sample - loss: 0.2135 - acc: 0.9223\n",
      "Epoch 2/2\n",
      "80664/80664 [==============================] - 1s 11us/sample - loss: 0.0756 - acc: 0.9766\n",
      "40332/40332 [==============================] - 0s 8us/sample - loss: 0.0852 - acc: 0.9802\n",
      "Epoch 1/2\n",
      "80664/80664 [==============================] - 1s 14us/sample - loss: 0.1765 - acc: 0.9416\n",
      "Epoch 2/2\n",
      "80664/80664 [==============================] - 1s 12us/sample - loss: 0.0740 - acc: 0.9777\n",
      "40332/40332 [==============================] - 0s 8us/sample - loss: 0.0643 - acc: 0.9792\n",
      "Epoch 1/2\n",
      "80664/80664 [==============================] - 2s 19us/sample - loss: 0.3112 - acc: 0.9155\n",
      "Epoch 2/2\n",
      "80664/80664 [==============================] - 1s 13us/sample - loss: 0.0850 - acc: 0.9704\n",
      "40332/40332 [==============================] - 0s 8us/sample - loss: 0.0696 - acc: 0.9801\n",
      "Epoch 1/2\n",
      "80664/80664 [==============================] - 1s 15us/sample - loss: 0.2105 - acc: 0.9370\n",
      "Epoch 2/2\n",
      "80664/80664 [==============================] - 1s 14us/sample - loss: 0.0794 - acc: 0.9754\n",
      "40332/40332 [==============================] - 0s 8us/sample - loss: 0.0814 - acc: 0.9791\n",
      "Epoch 1/2\n",
      "80664/80664 [==============================] - 1s 16us/sample - loss: 0.2443 - acc: 0.9179\n",
      "Epoch 2/2\n",
      "80664/80664 [==============================] - 1s 14us/sample - loss: 0.1026 - acc: 0.9616\n",
      "40332/40332 [==============================] - 0s 8us/sample - loss: 0.0857 - acc: 0.9714\n",
      "Epoch 1/2\n",
      "80664/80664 [==============================] - 1s 16us/sample - loss: 0.3558 - acc: 0.8857\n",
      "Epoch 2/2\n",
      "80664/80664 [==============================] - 1s 15us/sample - loss: 0.0844 - acc: 0.9746\n",
      "40332/40332 [==============================] - 0s 9us/sample - loss: 0.0727 - acc: 0.9768\n",
      "Epoch 1/3\n",
      "80664/80664 [==============================] - 1s 15us/sample - loss: 0.1775 - acc: 0.9412\n",
      "Epoch 2/3\n",
      "80664/80664 [==============================] - 1s 13us/sample - loss: 0.0759 - acc: 0.9763\n",
      "Epoch 3/3\n",
      "80664/80664 [==============================] - 1s 14us/sample - loss: 0.0607 - acc: 0.9799\n",
      "40332/40332 [==============================] - 0s 10us/sample - loss: 0.0706 - acc: 0.9798\n",
      "Epoch 1/3\n",
      "80664/80664 [==============================] - 1s 17us/sample - loss: 0.2505 - acc: 0.9138\n",
      "Epoch 2/3\n",
      "80664/80664 [==============================] - 1s 16us/sample - loss: 0.0796 - acc: 0.9756\n",
      "Epoch 3/3\n",
      "80664/80664 [==============================] - 1s 16us/sample - loss: 0.0656 - acc: 0.9797\n",
      "40332/40332 [==============================] - 0s 10us/sample - loss: 0.0623 - acc: 0.9799\n",
      "Epoch 1/3\n",
      "80664/80664 [==============================] - 1s 17us/sample - loss: 0.2063 - acc: 0.9433\n",
      "Epoch 2/3\n",
      "80664/80664 [==============================] - 1s 16us/sample - loss: 0.0751 - acc: 0.9779\n",
      "Epoch 3/3\n",
      "80664/80664 [==============================] - 1s 16us/sample - loss: 0.0612 - acc: 0.9808\n",
      "40332/40332 [==============================] - 0s 9us/sample - loss: 0.0546 - acc: 0.9818\n",
      "Epoch 1/3\n",
      "80664/80664 [==============================] - 1s 18us/sample - loss: 0.2040 - acc: 0.9313\n",
      "Epoch 2/3\n",
      "80664/80664 [==============================] - 1s 17us/sample - loss: 0.0743 - acc: 0.9766\n",
      "Epoch 3/3\n",
      "80664/80664 [==============================] - 1s 17us/sample - loss: 0.0614 - acc: 0.9802\n",
      "40332/40332 [==============================] - 0s 11us/sample - loss: 0.0617 - acc: 0.9810\n",
      "Epoch 1/3\n",
      "80664/80664 [==============================] - 1s 18us/sample - loss: 0.2662 - acc: 0.9203\n",
      "Epoch 2/3\n",
      "80664/80664 [==============================] - 1s 17us/sample - loss: 0.0815 - acc: 0.9737\n",
      "Epoch 3/3\n",
      "80664/80664 [==============================] - 1s 18us/sample - loss: 0.0608 - acc: 0.9804\n",
      "40332/40332 [==============================] - 0s 11us/sample - loss: 0.0578 - acc: 0.9807\n",
      "Epoch 1/3\n",
      "80664/80664 [==============================] - 2s 20us/sample - loss: 0.2886 - acc: 0.9016\n",
      "Epoch 2/3\n",
      "80664/80664 [==============================] - 1s 17us/sample - loss: 0.0923 - acc: 0.9739\n",
      "Epoch 3/3\n",
      "80664/80664 [==============================] - 1s 17us/sample - loss: 0.0674 - acc: 0.9795\n",
      "40332/40332 [==============================] - 0s 10us/sample - loss: 0.0601 - acc: 0.9804\n",
      "80664/80664 [==============================] - 1s 10us/sample - loss: 0.3223 - acc: 0.8941\n",
      "40332/40332 [==============================] - 0s 7us/sample - loss: 0.1341 - acc: 0.9500\n",
      "80664/80664 [==============================] - 1s 10us/sample - loss: 0.3861 - acc: 0.8907\n",
      "40332/40332 [==============================] - 0s 7us/sample - loss: 0.1223 - acc: 0.9579\n",
      "80664/80664 [==============================] - 1s 11us/sample - loss: 0.3850 - acc: 0.8993\n",
      "40332/40332 [==============================] - 0s 7us/sample - loss: 0.1184 - acc: 0.9606\n",
      "80664/80664 [==============================] - 1s 11us/sample - loss: 0.3660 - acc: 0.8924\n",
      "40332/40332 [==============================] - 0s 8us/sample - loss: 0.1314 - acc: 0.9643\n",
      "80664/80664 [==============================] - 1s 12us/sample - loss: 0.4127 - acc: 0.8837\n",
      "40332/40332 [==============================] - 0s 8us/sample - loss: 0.1595 - acc: 0.9486\n",
      "80664/80664 [==============================] - 1s 11us/sample - loss: 0.3146 - acc: 0.8905\n",
      "40332/40332 [==============================] - 0s 8us/sample - loss: 0.1382 - acc: 0.9503\n",
      "Epoch 1/2\n",
      "80664/80664 [==============================] - 1s 12us/sample - loss: 0.2732 - acc: 0.9278\n",
      "Epoch 2/2\n",
      "80664/80664 [==============================] - 1s 10us/sample - loss: 0.0879 - acc: 0.9730\n",
      "40332/40332 [==============================] - 0s 8us/sample - loss: 0.0754 - acc: 0.9786\n",
      "Epoch 1/2\n",
      "80664/80664 [==============================] - 1s 12us/sample - loss: 0.4352 - acc: 0.8742\n",
      "Epoch 2/2\n",
      "80664/80664 [==============================] - 1s 10us/sample - loss: 0.0959 - acc: 0.9713\n",
      "40332/40332 [==============================] - 0s 8us/sample - loss: 0.0773 - acc: 0.9779\n",
      "Epoch 1/2\n",
      "80664/80664 [==============================] - 1s 12us/sample - loss: 0.2927 - acc: 0.9011\n",
      "Epoch 2/2\n",
      "80664/80664 [==============================] - 1s 10us/sample - loss: 0.0963 - acc: 0.9727\n",
      "40332/40332 [==============================] - 0s 9us/sample - loss: 0.0731 - acc: 0.9768\n",
      "Epoch 1/2\n",
      "80664/80664 [==============================] - 1s 13us/sample - loss: 0.3579 - acc: 0.8879\n",
      "Epoch 2/2\n",
      "80664/80664 [==============================] - 1s 10us/sample - loss: 0.1124 - acc: 0.9651\n",
      "40332/40332 [==============================] - 0s 9us/sample - loss: 0.1232 - acc: 0.9729\n",
      "Epoch 1/2\n",
      "80664/80664 [==============================] - 1s 13us/sample - loss: 0.2775 - acc: 0.9135\n",
      "Epoch 2/2\n",
      "80664/80664 [==============================] - 1s 11us/sample - loss: 0.0993 - acc: 0.9631\n",
      "40332/40332 [==============================] - 0s 9us/sample - loss: 0.0888 - acc: 0.9695\n",
      "Epoch 1/2\n",
      "80664/80664 [==============================] - 1s 13us/sample - loss: 0.3664 - acc: 0.8894\n",
      "Epoch 2/2\n",
      "80664/80664 [==============================] - 1s 11us/sample - loss: 0.1211 - acc: 0.9542\n",
      "40332/40332 [==============================] - 0s 10us/sample - loss: 0.0954 - acc: 0.9637\n",
      "Epoch 1/3\n",
      "80664/80664 [==============================] - 1s 13us/sample - loss: 0.3008 - acc: 0.8790\n",
      "Epoch 2/3\n",
      "80664/80664 [==============================] - 1s 11us/sample - loss: 0.0851 - acc: 0.9729\n",
      "Epoch 3/3\n",
      "80664/80664 [==============================] - 1s 11us/sample - loss: 0.0667 - acc: 0.9787\n",
      "40332/40332 [==============================] - 0s 10us/sample - loss: 0.0771 - acc: 0.9815\n",
      "Epoch 1/3\n",
      "80664/80664 [==============================] - 1s 13us/sample - loss: 0.3215 - acc: 0.9007\n",
      "Epoch 2/3\n",
      "80664/80664 [==============================] - 1s 10us/sample - loss: 0.0929 - acc: 0.9714\n",
      "Epoch 3/3\n",
      "80664/80664 [==============================] - 1s 11us/sample - loss: 0.0687 - acc: 0.9798\n",
      "40332/40332 [==============================] - 0s 10us/sample - loss: 0.0690 - acc: 0.9807\n",
      "Epoch 1/3\n",
      "80664/80664 [==============================] - 1s 14us/sample - loss: 0.2774 - acc: 0.9042\n",
      "Epoch 2/3\n",
      "80664/80664 [==============================] - 1s 11us/sample - loss: 0.1000 - acc: 0.9622\n",
      "Epoch 3/3\n",
      "80664/80664 [==============================] - 1s 12us/sample - loss: 0.0759 - acc: 0.9745\n",
      "40332/40332 [==============================] - 0s 10us/sample - loss: 0.0663 - acc: 0.9769\n",
      "Epoch 1/3\n",
      "80664/80664 [==============================] - 1s 14us/sample - loss: 0.4626 - acc: 0.8512\n",
      "Epoch 2/3\n",
      "80664/80664 [==============================] - 1s 12us/sample - loss: 0.1131 - acc: 0.9617\n",
      "Epoch 3/3\n",
      "80664/80664 [==============================] - 1s 12us/sample - loss: 0.0806 - acc: 0.9755\n",
      "40332/40332 [==============================] - 0s 10us/sample - loss: 0.0721 - acc: 0.9786\n",
      "Epoch 1/3\n",
      "80664/80664 [==============================] - 1s 14us/sample - loss: 0.5546 - acc: 0.7315\n",
      "Epoch 2/3\n",
      "80664/80664 [==============================] - 1s 12us/sample - loss: 0.1026 - acc: 0.9707\n",
      "Epoch 3/3\n",
      "80664/80664 [==============================] - 1s 12us/sample - loss: 0.0746 - acc: 0.9782\n",
      "40332/40332 [==============================] - 0s 10us/sample - loss: 0.0703 - acc: 0.9774\n",
      "Epoch 1/3\n",
      "80664/80664 [==============================] - 1s 15us/sample - loss: 0.3863 - acc: 0.8857\n",
      "Epoch 2/3\n",
      "80664/80664 [==============================] - 1s 12us/sample - loss: 0.1168 - acc: 0.9635\n",
      "Epoch 3/3\n",
      "80664/80664 [==============================] - 1s 12us/sample - loss: 0.0819 - acc: 0.9760\n",
      "40332/40332 [==============================] - 0s 11us/sample - loss: 0.0720 - acc: 0.9783\n",
      "Epoch 1/3\n",
      "120996/120996 [==============================] - 3s 25us/sample - loss: 0.1753 - acc: 0.9451\n",
      "Epoch 2/3\n",
      "120996/120996 [==============================] - 3s 23us/sample - loss: 0.0655 - acc: 0.9796\n",
      "Epoch 3/3\n",
      "120996/120996 [==============================] - 3s 24us/sample - loss: 0.0549 - acc: 0.9811\n",
      "Best: 0.980710 using {'batch_size': 64, 'epochs': 3, 'optimizer': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(optimizer='rmsprop'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=78, activation='relu')) \n",
    "    model.add(Dense(8,  activation='relu')) \n",
    "    model.add(Dense(6,  activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) \n",
    "    return model\n",
    "train25y = train25[78]\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model)\n",
    "# grid search epochs, batch size and optimizer\n",
    "optimizers = ['rmsprop', 'adam']\n",
    "epochs = numpy.array([1, 2, 3])\n",
    "batches = numpy.array([64,128])\n",
    "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid_result = grid.fit(train25xen, train25y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from above we find the best parameter are:'batch_size': 64, 'epochs': 3, 'optimizer': 'adam'. However, the results of last run was:'batch_size': 64, 'epochs': 3, 'optimizer': 'rmsprop',we finally chose the previous one. And below is the 25% training dataset which is labeled trained with the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "120996/120996 [==============================] - 3s 25us/sample - loss: 0.1484 - acc: 0.9509\n",
      "Epoch 2/3\n",
      "120996/120996 [==============================] - 3s 23us/sample - loss: 0.0604 - acc: 0.9804\n",
      "Epoch 3/3\n",
      "120996/120996 [==============================] - 3s 23us/sample - loss: 0.0531 - acc: 0.9814\n",
      "120996/120996 [==============================] - 3s 25us/sample - loss: 0.0501 - acc: 0.9826\n",
      "\n",
      "acc: 98.26%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=78, activation='relu'))\n",
    "model.add(Dense(8,  activation='relu'))\n",
    "model.add(Dense(6,  activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.fit(train25xen, train25y, epochs=3, batch_size=64)\n",
    "trainscores = model.evaluate(train25xen, train25y)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], trainscores[1]*100))\n",
    "    #testscores = model.evaluate(testxen, testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predtest = model.predict(testxen)\n",
    "predtesty = predtest.argmax(axis=1)\n",
    "pickle.dump(predtesty, open( \"predtest.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reference:\n",
    "    Autoencoder:\n",
    "        https://yishuihancheng.blog.csdn.net/article/details/112292291?utm_term=%E8%87%AA%E5%8A%A8%E7%BC%96%E7%A0%81%E5%99%A8%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96&utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~all~sobaiduweb~default-1-112292291&spm=3001.4430\n",
    "    Hyper-parameter tuning: \n",
    "        https://cnbeining.github.io/deep-learning-with-python-cn/3-multi-layer-perceptrons/ch9-use-keras-models-with-scikit-learn-for-general-machine-learning.html\n",
    "    source code:\n",
    "        https://github.com/sgamage2/dl_ids_survey\n",
    "    NN:\n",
    "        https://dsbristol.github.io/dst/coursebook/09.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
